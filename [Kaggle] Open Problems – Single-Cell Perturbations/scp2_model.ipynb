{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base dataset\n",
    "\n",
    "de_train = pd.read_parquet(\"/home/aiuser/taeuk/open-problems-single-cell-perturbations/de_train.parquet\")\n",
    "id_map = pd.read_csv(\"/home/aiuser/taeuk/open-problems-single-cell-perturbations/id_map.csv\")\n",
    "submission = pd.read_csv(\"/home/aiuser/taeuk/open-problems-single-cell-perturbations/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means : [0.36676917 0.25095126 0.1482586  0.15573331 0.68427495]\n",
      " stds : [1.63469675 1.18865995 2.27540959 2.18409453 3.18920633]\n"
     ]
    }
   ],
   "source": [
    "gene_means = de_train.iloc[:, 5:].mean(axis=0).values\n",
    "gene_stds = de_train.iloc[:, 5:].std(axis=0).values\n",
    "\n",
    "print(\"means :\", gene_means[:5])\n",
    "print(\" stds :\", gene_stds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a92a6c387d40ac90dfd5b3f19bd6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "alpha = 0.05\n",
    "is_normals = []\n",
    "for i in tqdm(range(5, de_train.shape[1])):\n",
    "    _, p_value = shapiro(de_train.iloc[:, 5])\n",
    "    is_normals.append(int(p_value >= alpha))\n",
    "sum(is_normals) / len(is_normals) # 정규성은 없는듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell & compound dictionary\n",
    "\n",
    "cell_type_de_train = sorted(de_train.cell_type.unique())\n",
    "sm_name_de_train = sorted(de_train.sm_name.unique())\n",
    "\n",
    "cell_type_dict = {cell_type_de_train[i]:i for i in range(len(cell_type_de_train))}\n",
    "sm_name_dict = {sm_name_de_train[i]:i for i in range(len(sm_name_de_train))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_type</th>\n",
       "      <th>sm_name</th>\n",
       "      <th>sm_lincs_id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>control</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NK cells</td>\n",
       "      <td>Clotrimazole</td>\n",
       "      <td>LSM-5341</td>\n",
       "      <td>Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.104720</td>\n",
       "      <td>-0.077524</td>\n",
       "      <td>-1.625596</td>\n",
       "      <td>-0.144545</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227781</td>\n",
       "      <td>-0.010752</td>\n",
       "      <td>-0.023881</td>\n",
       "      <td>0.674536</td>\n",
       "      <td>-0.453068</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>-0.094959</td>\n",
       "      <td>0.034127</td>\n",
       "      <td>0.221377</td>\n",
       "      <td>0.368755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T cells CD4+</td>\n",
       "      <td>Clotrimazole</td>\n",
       "      <td>LSM-5341</td>\n",
       "      <td>Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.915953</td>\n",
       "      <td>-0.884380</td>\n",
       "      <td>0.371834</td>\n",
       "      <td>-0.081677</td>\n",
       "      <td>-0.498266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.494985</td>\n",
       "      <td>-0.303419</td>\n",
       "      <td>0.304955</td>\n",
       "      <td>-0.333905</td>\n",
       "      <td>-0.315516</td>\n",
       "      <td>-0.369626</td>\n",
       "      <td>-0.095079</td>\n",
       "      <td>0.704780</td>\n",
       "      <td>1.096702</td>\n",
       "      <td>-0.869887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T cells CD8+</td>\n",
       "      <td>Clotrimazole</td>\n",
       "      <td>LSM-5341</td>\n",
       "      <td>Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.387721</td>\n",
       "      <td>-0.305378</td>\n",
       "      <td>0.567777</td>\n",
       "      <td>0.303895</td>\n",
       "      <td>-0.022653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119422</td>\n",
       "      <td>-0.033608</td>\n",
       "      <td>-0.153123</td>\n",
       "      <td>0.183597</td>\n",
       "      <td>-0.555678</td>\n",
       "      <td>-1.494789</td>\n",
       "      <td>-0.213550</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.078439</td>\n",
       "      <td>-0.259365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T regulatory cells</td>\n",
       "      <td>Clotrimazole</td>\n",
       "      <td>LSM-5341</td>\n",
       "      <td>Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.232893</td>\n",
       "      <td>0.129029</td>\n",
       "      <td>0.336897</td>\n",
       "      <td>0.486946</td>\n",
       "      <td>0.767661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451679</td>\n",
       "      <td>0.704643</td>\n",
       "      <td>0.015468</td>\n",
       "      <td>-0.103868</td>\n",
       "      <td>0.865027</td>\n",
       "      <td>0.189114</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>-0.048233</td>\n",
       "      <td>0.216139</td>\n",
       "      <td>-0.085024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NK cells</td>\n",
       "      <td>Mometasone Furoate</td>\n",
       "      <td>LSM-3349</td>\n",
       "      <td>C[C@@H]1C[C@H]2[C@@H]3CCC4=CC(=O)C=C[C@]4(C)[C...</td>\n",
       "      <td>False</td>\n",
       "      <td>4.290652</td>\n",
       "      <td>-0.063864</td>\n",
       "      <td>-0.017443</td>\n",
       "      <td>-0.541154</td>\n",
       "      <td>0.570982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758474</td>\n",
       "      <td>0.510762</td>\n",
       "      <td>0.607401</td>\n",
       "      <td>-0.123059</td>\n",
       "      <td>0.214366</td>\n",
       "      <td>0.487838</td>\n",
       "      <td>-0.819775</td>\n",
       "      <td>0.112365</td>\n",
       "      <td>-0.122193</td>\n",
       "      <td>0.676629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>T regulatory cells</td>\n",
       "      <td>Atorvastatin</td>\n",
       "      <td>LSM-5771</td>\n",
       "      <td>CC(C)c1c(C(=O)Nc2ccccc2)c(-c2ccccc2)c(-c2ccc(F...</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.014372</td>\n",
       "      <td>-0.122464</td>\n",
       "      <td>-0.456366</td>\n",
       "      <td>-0.147894</td>\n",
       "      <td>-0.545382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.549987</td>\n",
       "      <td>-2.200925</td>\n",
       "      <td>0.359806</td>\n",
       "      <td>1.073983</td>\n",
       "      <td>0.356939</td>\n",
       "      <td>-0.029603</td>\n",
       "      <td>-0.528817</td>\n",
       "      <td>0.105138</td>\n",
       "      <td>0.491015</td>\n",
       "      <td>-0.979951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>NK cells</td>\n",
       "      <td>Riociguat</td>\n",
       "      <td>LSM-45758</td>\n",
       "      <td>COC(=O)N(C)c1c(N)nc(-c2nn(Cc3ccccc3F)c3ncccc23...</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.455549</td>\n",
       "      <td>0.188181</td>\n",
       "      <td>0.595734</td>\n",
       "      <td>-0.100299</td>\n",
       "      <td>0.786192</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.236905</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>-0.197569</td>\n",
       "      <td>-0.175307</td>\n",
       "      <td>0.101391</td>\n",
       "      <td>1.028394</td>\n",
       "      <td>0.034144</td>\n",
       "      <td>-0.231642</td>\n",
       "      <td>1.023994</td>\n",
       "      <td>-0.064760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>T cells CD4+</td>\n",
       "      <td>Riociguat</td>\n",
       "      <td>LSM-45758</td>\n",
       "      <td>COC(=O)N(C)c1c(N)nc(-c2nn(Cc3ccccc3F)c3ncccc23...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.338168</td>\n",
       "      <td>-0.109079</td>\n",
       "      <td>0.270182</td>\n",
       "      <td>-0.436586</td>\n",
       "      <td>-0.069476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077579</td>\n",
       "      <td>-1.101637</td>\n",
       "      <td>0.457201</td>\n",
       "      <td>0.535184</td>\n",
       "      <td>-0.198404</td>\n",
       "      <td>-0.005004</td>\n",
       "      <td>0.552810</td>\n",
       "      <td>-0.209077</td>\n",
       "      <td>0.389751</td>\n",
       "      <td>-0.337082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>T cells CD8+</td>\n",
       "      <td>Riociguat</td>\n",
       "      <td>LSM-45758</td>\n",
       "      <td>COC(=O)N(C)c1c(N)nc(-c2nn(Cc3ccccc3F)c3ncccc23...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.101138</td>\n",
       "      <td>-0.409724</td>\n",
       "      <td>-0.606292</td>\n",
       "      <td>-0.071300</td>\n",
       "      <td>-0.001789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>-0.893093</td>\n",
       "      <td>-1.003029</td>\n",
       "      <td>-0.080367</td>\n",
       "      <td>-0.076604</td>\n",
       "      <td>0.024849</td>\n",
       "      <td>0.012862</td>\n",
       "      <td>-0.029684</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>-1.733112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>T regulatory cells</td>\n",
       "      <td>Riociguat</td>\n",
       "      <td>LSM-45758</td>\n",
       "      <td>COC(=O)N(C)c1c(N)nc(-c2nn(Cc3ccccc3F)c3ncccc23...</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.757116</td>\n",
       "      <td>0.085910</td>\n",
       "      <td>-0.730025</td>\n",
       "      <td>-1.367801</td>\n",
       "      <td>-0.695944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232343</td>\n",
       "      <td>-2.247816</td>\n",
       "      <td>-0.346036</td>\n",
       "      <td>-0.919567</td>\n",
       "      <td>-1.131372</td>\n",
       "      <td>-0.120252</td>\n",
       "      <td>-0.064537</td>\n",
       "      <td>-0.603280</td>\n",
       "      <td>-0.098041</td>\n",
       "      <td>-0.750681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows × 18216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              cell_type             sm_name sm_lincs_id  \\\n",
       "0              NK cells        Clotrimazole    LSM-5341   \n",
       "1          T cells CD4+        Clotrimazole    LSM-5341   \n",
       "2          T cells CD8+        Clotrimazole    LSM-5341   \n",
       "3    T regulatory cells        Clotrimazole    LSM-5341   \n",
       "4              NK cells  Mometasone Furoate    LSM-3349   \n",
       "..                  ...                 ...         ...   \n",
       "609  T regulatory cells        Atorvastatin    LSM-5771   \n",
       "610            NK cells           Riociguat   LSM-45758   \n",
       "611        T cells CD4+           Riociguat   LSM-45758   \n",
       "612        T cells CD8+           Riociguat   LSM-45758   \n",
       "613  T regulatory cells           Riociguat   LSM-45758   \n",
       "\n",
       "                                                SMILES  control      A1BG  \\\n",
       "0               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1    False  0.104720   \n",
       "1               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1    False  0.915953   \n",
       "2               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1    False -0.387721   \n",
       "3               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1    False  0.232893   \n",
       "4    C[C@@H]1C[C@H]2[C@@H]3CCC4=CC(=O)C=C[C@]4(C)[C...    False  4.290652   \n",
       "..                                                 ...      ...       ...   \n",
       "609  CC(C)c1c(C(=O)Nc2ccccc2)c(-c2ccccc2)c(-c2ccc(F...    False -0.014372   \n",
       "610  COC(=O)N(C)c1c(N)nc(-c2nn(Cc3ccccc3F)c3ncccc23...    False -0.455549   \n",
       "611  COC(=O)N(C)c1c(N)nc(-c2nn(Cc3ccccc3F)c3ncccc23...    False  0.338168   \n",
       "612  COC(=O)N(C)c1c(N)nc(-c2nn(Cc3ccccc3F)c3ncccc23...    False  0.101138   \n",
       "613  COC(=O)N(C)c1c(N)nc(-c2nn(Cc3ccccc3F)c3ncccc23...    False -0.757116   \n",
       "\n",
       "     A1BG-AS1       A2M   A2M-AS1     A2MP1  ...      ZUP1      ZW10  \\\n",
       "0   -0.077524 -1.625596 -0.144545  0.143555  ... -0.227781 -0.010752   \n",
       "1   -0.884380  0.371834 -0.081677 -0.498266  ... -0.494985 -0.303419   \n",
       "2   -0.305378  0.567777  0.303895 -0.022653  ... -0.119422 -0.033608   \n",
       "3    0.129029  0.336897  0.486946  0.767661  ...  0.451679  0.704643   \n",
       "4   -0.063864 -0.017443 -0.541154  0.570982  ...  0.758474  0.510762   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "609 -0.122464 -0.456366 -0.147894 -0.545382  ... -0.549987 -2.200925   \n",
       "610  0.188181  0.595734 -0.100299  0.786192  ... -1.236905  0.003854   \n",
       "611 -0.109079  0.270182 -0.436586 -0.069476  ...  0.077579 -1.101637   \n",
       "612 -0.409724 -0.606292 -0.071300 -0.001789  ...  0.005951 -0.893093   \n",
       "613  0.085910 -0.730025 -1.367801 -0.695944  ...  0.232343 -2.247816   \n",
       "\n",
       "       ZWILCH     ZWINT      ZXDA      ZXDB      ZXDC    ZYG11B       ZYX  \\\n",
       "0   -0.023881  0.674536 -0.453068  0.005164 -0.094959  0.034127  0.221377   \n",
       "1    0.304955 -0.333905 -0.315516 -0.369626 -0.095079  0.704780  1.096702   \n",
       "2   -0.153123  0.183597 -0.555678 -1.494789 -0.213550  0.415768  0.078439   \n",
       "3    0.015468 -0.103868  0.865027  0.189114  0.224700 -0.048233  0.216139   \n",
       "4    0.607401 -0.123059  0.214366  0.487838 -0.819775  0.112365 -0.122193   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "609  0.359806  1.073983  0.356939 -0.029603 -0.528817  0.105138  0.491015   \n",
       "610 -0.197569 -0.175307  0.101391  1.028394  0.034144 -0.231642  1.023994   \n",
       "611  0.457201  0.535184 -0.198404 -0.005004  0.552810 -0.209077  0.389751   \n",
       "612 -1.003029 -0.080367 -0.076604  0.024849  0.012862 -0.029684  0.005506   \n",
       "613 -0.346036 -0.919567 -1.131372 -0.120252 -0.064537 -0.603280 -0.098041   \n",
       "\n",
       "        ZZEF1  \n",
       "0    0.368755  \n",
       "1   -0.869887  \n",
       "2   -0.259365  \n",
       "3   -0.085024  \n",
       "4    0.676629  \n",
       "..        ...  \n",
       "609 -0.979951  \n",
       "610 -0.064760  \n",
       "611 -0.337082  \n",
       "612 -1.733112  \n",
       "613 -0.750681  \n",
       "\n",
       "[614 rows x 18216 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_name</th>\n",
       "      <th>#</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>+</th>\n",
       "      <th>-</th>\n",
       "      <th>/</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>S</th>\n",
       "      <th>[</th>\n",
       "      <th>\\</th>\n",
       "      <th>]</th>\n",
       "      <th>c</th>\n",
       "      <th>l</th>\n",
       "      <th>n</th>\n",
       "      <th>o</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clotrimazole</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mometasone Furoate</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Idelalisib</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vandetanib</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bosutinib</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              sm_name  #  (  )  +  -  /  1  2  3  ...  S  [  \\  ]   c  l  n  \\\n",
       "0        Clotrimazole  0  2  2  0  0  0  8  0  0  ...  0  0  0  0  21  1  2   \n",
       "1  Mometasone Furoate  0  8  8  0  0  0  4  2  2  ...  0  8  0  8   4  2  0   \n",
       "2          Idelalisib  0  3  3  0  1  0  6  4  0  ...  0  2  0  2  19  0  6   \n",
       "3          Vandetanib  0  3  3  0  0  0  4  2  2  ...  0  0  0  0  14  0  2   \n",
       "4           Bosutinib  1  6  6  0  0  0  2  2  2  ...  0  0  0  0  15  2  1   \n",
       "\n",
       "   o  r  s  \n",
       "0  0  0  0  \n",
       "1  1  0  0  \n",
       "2  0  0  0  \n",
       "3  0  1  0  \n",
       "4  0  0  0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compound decomposition \n",
    "\n",
    "# smiles = list(de_train.SMILES.unique())\n",
    "# voc = []\n",
    "\n",
    "# r = re.compile(\".\")\n",
    "# for sm in smiles:\n",
    "#     voc += list(set(r.findall(sm)))\n",
    "# voc = list(set(voc))\n",
    "# voc.sort()\n",
    "\n",
    "# smile = pd.DataFrame(np.zeros((len(smiles), len(voc))).astype(int))\n",
    "# smile.columns = voc\n",
    "# for i in range(smile.shape[0]):\n",
    "#     for ele in r.findall(smiles[i]):\n",
    "#         smile[ele][i] += 1\n",
    "# smile = pd.DataFrame(smiles).join(smile)\n",
    "# smile.columns = [\"SMILES\"] + list(smile.columns[1:])\n",
    "# sm_id = de_train.iloc[:, 1:4].drop(\"sm_lincs_id\", axis=1).drop_duplicates()\n",
    "# smile = sm_id.merge(smile, how=\"left\", on=\"SMILES\")\n",
    "# smile.drop(\"SMILES\", axis=1, inplace=True)\n",
    "# smile.to_csv(\"/home/aiuser/taeuk/open-problems-single-cell-perturbations/smile.csv\", header=True, index=False)\n",
    "smile = pd.read_csv(\"/home/aiuser/taeuk/open-problems-single-cell-perturbations/smile.csv\")\n",
    "display(de_train)\n",
    "smile.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B cells</th>\n",
       "      <th>Myeloid cells</th>\n",
       "      <th>NK cells</th>\n",
       "      <th>T cells CD4+</th>\n",
       "      <th>T cells CD8+</th>\n",
       "      <th>T regulatory cells</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B cells</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647261</td>\n",
       "      <td>0.788213</td>\n",
       "      <td>0.485822</td>\n",
       "      <td>-0.138323</td>\n",
       "      <td>0.066281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Myeloid cells</th>\n",
       "      <td>0.647261</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.609587</td>\n",
       "      <td>0.323018</td>\n",
       "      <td>-0.194258</td>\n",
       "      <td>0.191862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NK cells</th>\n",
       "      <td>0.788213</td>\n",
       "      <td>0.609587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.512614</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.238791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T cells CD4+</th>\n",
       "      <td>0.485822</td>\n",
       "      <td>0.323018</td>\n",
       "      <td>0.512614</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.302101</td>\n",
       "      <td>0.123189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T cells CD8+</th>\n",
       "      <td>-0.138323</td>\n",
       "      <td>-0.194258</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.302101</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.075527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                B cells  Myeloid cells  NK cells  T cells CD4+  T cells CD8+  \\\n",
       "cell_type                                                                      \n",
       "B cells        1.000000       0.647261  0.788213      0.485822     -0.138323   \n",
       "Myeloid cells  0.647261       1.000000  0.609587      0.323018     -0.194258   \n",
       "NK cells       0.788213       0.609587  1.000000      0.512614      0.003340   \n",
       "T cells CD4+   0.485822       0.323018  0.512614      1.000000      0.302101   \n",
       "T cells CD8+  -0.138323      -0.194258  0.003340      0.302101      1.000000   \n",
       "\n",
       "               T regulatory cells  \n",
       "cell_type                          \n",
       "B cells                  0.066281  \n",
       "Myeloid cells            0.191862  \n",
       "NK cells                 0.238791  \n",
       "T cells CD4+             0.123189  \n",
       "T cells CD8+            -0.075527  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6-yl)pyrimidin-2-amine</th>\n",
       "      <th>ABT-199 (GDC-0199)</th>\n",
       "      <th>ABT737</th>\n",
       "      <th>AMD-070 (hydrochloride)</th>\n",
       "      <th>AT 7867</th>\n",
       "      <th>AT13387</th>\n",
       "      <th>AVL-292</th>\n",
       "      <th>AZ628</th>\n",
       "      <th>AZD-8330</th>\n",
       "      <th>AZD3514</th>\n",
       "      <th>...</th>\n",
       "      <th>Tivozanib</th>\n",
       "      <th>Topotecan</th>\n",
       "      <th>Tosedostat</th>\n",
       "      <th>Trametinib</th>\n",
       "      <th>UNII-BXU45ZH6LI</th>\n",
       "      <th>Vandetanib</th>\n",
       "      <th>Vanoxerine</th>\n",
       "      <th>Vardenafil</th>\n",
       "      <th>Vorinostat</th>\n",
       "      <th>YK 4-279</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sm_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6-yl)pyrimidin-2-amine</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.188602</td>\n",
       "      <td>0.038083</td>\n",
       "      <td>0.190366</td>\n",
       "      <td>0.363480</td>\n",
       "      <td>0.560509</td>\n",
       "      <td>0.217774</td>\n",
       "      <td>0.269561</td>\n",
       "      <td>0.289016</td>\n",
       "      <td>0.074848</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153586</td>\n",
       "      <td>-0.135050</td>\n",
       "      <td>0.340408</td>\n",
       "      <td>0.404999</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>-0.116056</td>\n",
       "      <td>0.154220</td>\n",
       "      <td>-0.070970</td>\n",
       "      <td>0.281518</td>\n",
       "      <td>-0.028914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABT-199 (GDC-0199)</th>\n",
       "      <td>-0.188602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.284557</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>-0.163023</td>\n",
       "      <td>-0.177109</td>\n",
       "      <td>-0.004786</td>\n",
       "      <td>-0.120617</td>\n",
       "      <td>-0.139353</td>\n",
       "      <td>0.067591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099148</td>\n",
       "      <td>0.093487</td>\n",
       "      <td>-0.168061</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.173189</td>\n",
       "      <td>-0.126120</td>\n",
       "      <td>-0.046650</td>\n",
       "      <td>0.100247</td>\n",
       "      <td>-0.281967</td>\n",
       "      <td>0.206045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABT737</th>\n",
       "      <td>0.038083</td>\n",
       "      <td>0.284557</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.063961</td>\n",
       "      <td>0.071608</td>\n",
       "      <td>0.061823</td>\n",
       "      <td>-0.102238</td>\n",
       "      <td>0.186619</td>\n",
       "      <td>0.158490</td>\n",
       "      <td>0.267674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025967</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.081146</td>\n",
       "      <td>0.072904</td>\n",
       "      <td>0.642046</td>\n",
       "      <td>-0.083797</td>\n",
       "      <td>0.192162</td>\n",
       "      <td>-0.044400</td>\n",
       "      <td>-0.016190</td>\n",
       "      <td>0.132450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMD-070 (hydrochloride)</th>\n",
       "      <td>0.190366</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>-0.063961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>0.177195</td>\n",
       "      <td>0.163277</td>\n",
       "      <td>-0.026261</td>\n",
       "      <td>-0.005190</td>\n",
       "      <td>-0.077422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010943</td>\n",
       "      <td>0.033453</td>\n",
       "      <td>0.027434</td>\n",
       "      <td>0.276627</td>\n",
       "      <td>-0.205314</td>\n",
       "      <td>-0.115135</td>\n",
       "      <td>-0.027755</td>\n",
       "      <td>0.094514</td>\n",
       "      <td>-0.020659</td>\n",
       "      <td>0.180129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT 7867</th>\n",
       "      <td>0.363480</td>\n",
       "      <td>-0.163023</td>\n",
       "      <td>0.071608</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.294058</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>0.284242</td>\n",
       "      <td>0.289464</td>\n",
       "      <td>0.134293</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068767</td>\n",
       "      <td>-0.072275</td>\n",
       "      <td>0.307853</td>\n",
       "      <td>0.096939</td>\n",
       "      <td>0.162107</td>\n",
       "      <td>0.121511</td>\n",
       "      <td>0.235875</td>\n",
       "      <td>-0.097620</td>\n",
       "      <td>0.306202</td>\n",
       "      <td>-0.095428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6-yl)pyrimidin-2-amine  \\\n",
       "sm_name                                                                                                                    \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...                                           1.000000                      \n",
       "ABT-199 (GDC-0199)                                                                          -0.188602                      \n",
       "ABT737                                                                                       0.038083                      \n",
       "AMD-070 (hydrochloride)                                                                      0.190366                      \n",
       "AT 7867                                                                                      0.363480                      \n",
       "\n",
       "                                                    ABT-199 (GDC-0199)  \\\n",
       "sm_name                                                                  \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...           -0.188602   \n",
       "ABT-199 (GDC-0199)                                            1.000000   \n",
       "ABT737                                                        0.284557   \n",
       "AMD-070 (hydrochloride)                                       0.004126   \n",
       "AT 7867                                                      -0.163023   \n",
       "\n",
       "                                                      ABT737  \\\n",
       "sm_name                                                        \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...  0.038083   \n",
       "ABT-199 (GDC-0199)                                  0.284557   \n",
       "ABT737                                              1.000000   \n",
       "AMD-070 (hydrochloride)                            -0.063961   \n",
       "AT 7867                                             0.071608   \n",
       "\n",
       "                                                    AMD-070 (hydrochloride)  \\\n",
       "sm_name                                                                       \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...                 0.190366   \n",
       "ABT-199 (GDC-0199)                                                 0.004126   \n",
       "ABT737                                                            -0.063961   \n",
       "AMD-070 (hydrochloride)                                            1.000000   \n",
       "AT 7867                                                            0.021369   \n",
       "\n",
       "                                                     AT 7867   AT13387  \\\n",
       "sm_name                                                                  \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...  0.363480  0.560509   \n",
       "ABT-199 (GDC-0199)                                 -0.163023 -0.177109   \n",
       "ABT737                                              0.071608  0.061823   \n",
       "AMD-070 (hydrochloride)                             0.021369  0.177195   \n",
       "AT 7867                                             1.000000  0.294058   \n",
       "\n",
       "                                                     AVL-292     AZ628  \\\n",
       "sm_name                                                                  \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...  0.217774  0.269561   \n",
       "ABT-199 (GDC-0199)                                 -0.004786 -0.120617   \n",
       "ABT737                                             -0.102238  0.186619   \n",
       "AMD-070 (hydrochloride)                             0.163277 -0.026261   \n",
       "AT 7867                                             0.028044  0.284242   \n",
       "\n",
       "                                                    AZD-8330   AZD3514  ...  \\\n",
       "sm_name                                                                 ...   \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...  0.289016  0.074848  ...   \n",
       "ABT-199 (GDC-0199)                                 -0.139353  0.067591  ...   \n",
       "ABT737                                              0.158490  0.267674  ...   \n",
       "AMD-070 (hydrochloride)                            -0.005190 -0.077422  ...   \n",
       "AT 7867                                             0.289464  0.134293  ...   \n",
       "\n",
       "                                                    Tivozanib  Topotecan  \\\n",
       "sm_name                                                                    \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...  -0.153586  -0.135050   \n",
       "ABT-199 (GDC-0199)                                   0.099148   0.093487   \n",
       "ABT737                                               0.025967   0.000807   \n",
       "AMD-070 (hydrochloride)                             -0.010943   0.033453   \n",
       "AT 7867                                             -0.068767  -0.072275   \n",
       "\n",
       "                                                    Tosedostat  Trametinib  \\\n",
       "sm_name                                                                      \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...    0.340408    0.404999   \n",
       "ABT-199 (GDC-0199)                                   -0.168061    0.001327   \n",
       "ABT737                                                0.081146    0.072904   \n",
       "AMD-070 (hydrochloride)                               0.027434    0.276627   \n",
       "AT 7867                                               0.307853    0.096939   \n",
       "\n",
       "                                                    UNII-BXU45ZH6LI  \\\n",
       "sm_name                                                               \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...         0.003311   \n",
       "ABT-199 (GDC-0199)                                         0.173189   \n",
       "ABT737                                                     0.642046   \n",
       "AMD-070 (hydrochloride)                                   -0.205314   \n",
       "AT 7867                                                    0.162107   \n",
       "\n",
       "                                                    Vandetanib  Vanoxerine  \\\n",
       "sm_name                                                                      \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...   -0.116056    0.154220   \n",
       "ABT-199 (GDC-0199)                                   -0.126120   -0.046650   \n",
       "ABT737                                               -0.083797    0.192162   \n",
       "AMD-070 (hydrochloride)                              -0.115135   -0.027755   \n",
       "AT 7867                                               0.121511    0.235875   \n",
       "\n",
       "                                                    Vardenafil  Vorinostat  \\\n",
       "sm_name                                                                      \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6...   -0.070970    0.281518   \n",
       "ABT-199 (GDC-0199)                                    0.100247   -0.281967   \n",
       "ABT737                                               -0.044400   -0.016190   \n",
       "AMD-070 (hydrochloride)                               0.094514   -0.020659   \n",
       "AT 7867                                              -0.097620    0.306202   \n",
       "\n",
       "                                                    YK 4-279  \n",
       "sm_name                                                       \n",
       "5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6... -0.028914  \n",
       "ABT-199 (GDC-0199)                                  0.206045  \n",
       "ABT737                                              0.132450  \n",
       "AMD-070 (hydrochloride)                             0.180129  \n",
       "AT 7867                                            -0.095428  \n",
       "\n",
       "[5 rows x 146 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# correlation\n",
    "\n",
    "# gene_names = list(de_train.columns[5:])\n",
    "# de_melt = de_train.melt(id_vars=[\"sm_name\", \"cell_type\"], value_vars=gene_names)\n",
    "\n",
    "# de_pivot_cell = de_melt.pivot(index=[\"sm_name\",\"variable\"], columns=\"cell_type\", values=\"value\")\n",
    "# de_pivot_comp = de_melt.pivot(index=[\"cell_type\",\"variable\"], columns=\"sm_name\", values=\"value\")\n",
    "# de_corr_cell = de_pivot_cell.corr()\n",
    "# de_corr_comp = de_pivot_comp.corr()\n",
    "\n",
    "# de_corr_cell.to_csv(\"/home/aiuser/taeuk/open-problems-single-cell-perturbations/de_corr_cell.csv\", header=True, index=True)\n",
    "# de_corr_comp.to_csv(\"/home/aiuser/taeuk/open-problems-single-cell-perturbations/de_corr_comp.csv\", header=True, index=True)\n",
    "\n",
    "de_corr_cell = pd.read_csv(\"/home/aiuser/taeuk/open-problems-single-cell-perturbations/de_corr_cell.csv\", header=0, index_col=0)\n",
    "de_corr_comp = pd.read_csv(\"/home/aiuser/taeuk/open-problems-single-cell-perturbations/de_corr_comp.csv\", header=0, index_col=0)\n",
    "\n",
    "display(de_corr_cell.head())\n",
    "display(de_corr_comp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build custom dataset\n",
    "\n",
    "class SCPpretrain(Dataset):\n",
    "    def __init__(self, dataset, smile, cell_type_dict,\n",
    "                 de_corr_cell, de_corr_comp):\n",
    "        super(SCPpretrain, self).__init__()\n",
    "        self.x = dataset.iloc[:, :2]\n",
    "        self.smile = smile\n",
    "        self.cell_type_dict = cell_type_dict\n",
    "        self.de_corr_cell = de_corr_cell\n",
    "        self.de_corr_comp = de_corr_comp\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        cell, name = self.x.iloc[idx]\n",
    "        x_cell = self.cell_type_dict[cell]\n",
    "        \n",
    "        ele_val = self.smile.loc[self.smile.sm_name==name, :].values[0]\n",
    "        ele_bool = ele_val != 0\n",
    "        ele_idx = [i if ele_bool[i]==True else 0 for i in np.arange(ele_val.shape[0])]\n",
    "        x = [x_cell] + list(ele_idx[1:]) + list(ele_val[1:])\n",
    "        \n",
    "        y_cell = de_corr_cell.loc[cell].values\n",
    "        y_comp = de_corr_comp.loc[name].values\n",
    "        y = list(y_cell) + list(y_comp)\n",
    "        \n",
    "        return torch.tensor(x, dtype=torch.int64),\\\n",
    "               torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "        \n",
    "        \n",
    "class SCPset(Dataset):\n",
    "    def __init__(self, dataset, smile, cell_type_dict, normalize):\n",
    "        super(SCPset, self).__init__()\n",
    "        if dataset is None:\n",
    "            self.x = id_map.iloc[:, 1:]\n",
    "            self.y = None\n",
    "        else:\n",
    "            self.x = dataset.iloc[:, :2]\n",
    "            self.y = dataset.iloc[:, 5:]\n",
    "            if normalize:\n",
    "                self.means = self.y.mean(axis=0).values\n",
    "                self.stds = self.y.std(axis=0).values\n",
    "                #\n",
    "                #\n",
    "                self.y = (self.y - self.means)/self.stds\n",
    "        self.smile = smile\n",
    "        self.cell_type_dict = cell_type_dict\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        cell, name = self.x.iloc[idx]\n",
    "        x_cell = self.cell_type_dict[cell]\n",
    "        \n",
    "        ele_val = self.smile.loc[self.smile.sm_name==name, :].values[0]\n",
    "        ele_bool = ele_val != 0\n",
    "        ele_idx = [i if ele_bool[i]==True else 0 for i in np.arange(ele_val.shape[0])]\n",
    "        x = [x_cell] + list(ele_idx[1:]) + list(ele_val[1:])\n",
    "        \n",
    "        if self.y is None:\n",
    "            return torch.tensor(x, dtype=torch.int64)\n",
    "        else:\n",
    "            y = self.y.iloc[idx, :]\n",
    "            return torch.tensor(x, dtype=torch.int64),\\\n",
    "                    torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 65])\n",
      "torch.Size([128, 152])\n",
      "torch.Size([128, 65])\n",
      "torch.Size([128, 18211])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1915653/3917753283.py:65: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  torch.tensor(y, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# dataset check\n",
    "\n",
    "datap = DataLoader(SCPpretrain(de_train, smile, cell_type_dict,\n",
    "                 de_corr_cell, de_corr_comp), 128, False)\n",
    "xp, yp = next(iter(datap))\n",
    "print(xp.shape)\n",
    "print(yp.shape)\n",
    "\n",
    "data = DataLoader(SCPset(de_train, smile, cell_type_dict, True), 128, False)\n",
    "x, y = next(iter(data))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "\n",
    "class FinalLayer(nn.Module):\n",
    "    def __init__(self, dim_embed, device):\n",
    "        super(FinalLayer, self).__init__()\n",
    "        self.fc_v1 = nn.Linear(dim_embed, 18211, device=device)\n",
    "        self.fc_w1 = nn.Linear(dim_embed, 18211, device=device)\n",
    "        self.fc_v2 = nn.Linear(dim_embed, 18211, device=device)\n",
    "        self.fc_w2 = nn.Linear(dim_embed, 18211, device=device)\n",
    "    def forward(self, x):\n",
    "        v1 = self.fc_v1(x).tanh()\n",
    "        v2 = self.fc_v2(x).tanh()\n",
    "        w1 = self.fc_w1(x).sigmoid()\n",
    "        w2 = self.fc_w2(x).sigmoid()\n",
    "        return v1*w1 + v2*w2\n",
    "    \n",
    "class InterLayer(nn.Module):\n",
    "    def __init__(self, dim_embed, num_layers, drop_prob, device):\n",
    "        super(InterLayer, self).__init__()\n",
    "        self.main_encoder = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Linear(dim_embed*2*i, dim_embed*2*(i+1), device=device),\n",
    "                nn.BatchNorm1d(dim_embed*2*(i+1), device=device),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop_prob)\n",
    "            ) for i in range(1, num_layers+1)] )\n",
    "        \n",
    "        self.mu = nn.Linear(dim_embed*2*(num_layers+1),\n",
    "                            dim_embed*2*(num_layers+1), device=device)\n",
    "        self.var = nn.Linear(dim_embed*2*(num_layers+1),\n",
    "                             dim_embed*2*(num_layers+1), device=device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xout = self.main_encoder(x)\n",
    "        mu = self.mu(xout)\n",
    "        logvar = self.var(xout)\n",
    "        \n",
    "        std = torch.exp(logvar / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return xout, z, mu, logvar\n",
    "        \n",
    "class SCP2model(nn.Module):\n",
    "    def __init__(self, device, dim_embed, num_layers, drop_prob, log, pretrain):\n",
    "        super(SCP2model, self).__init__()\n",
    "        self.embed_cell = nn.Embedding(6, dim_embed, device=device)\n",
    "        self.embed_comp = nn.Embedding(33, dim_embed, padding_idx=0, device=device)\n",
    "        self.conv_comp = nn.Conv1d(32, 1, 3, 1, 1, bias=True, device=device)\n",
    "        self.inter_encoder = InterLayer(dim_embed, num_layers, drop_prob, device=device)\n",
    "        self.final_layer = FinalLayer(dim_embed*2*(num_layers+1), device=device)\n",
    "        self.log = log\n",
    "        self.pretrain = pretrain\n",
    "        \n",
    "    def calc_cell_vector(self, cell_idx):\n",
    "        return self.embed_cell(cell_idx)\n",
    "      \n",
    "    def calc_comp_vector(self, comp_idx, comp_val):\n",
    "        comp_val = (comp_val + 1).log() if self.log else comp_val\n",
    "        comp = self.embed_comp(comp_idx)\n",
    "        comp = comp * comp_val.unsqueeze(2)\n",
    "        comp = self.conv_comp(comp).squeeze()\n",
    "        return comp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hs = x.shape[1]-1\n",
    "        cell = self.calc_cell_vector(x[:, 0])\n",
    "        comp = self.calc_comp_vector(x[:, 1:hs//2+1], x[:, hs//2+1:])\n",
    "\n",
    "        if self.pretrain:\n",
    "            return cell, comp\n",
    "        else:\n",
    "            x_concat = torch.cat([cell, comp], dim=1)\n",
    "            xout, z, mu, logvar = self.inter_encoder(x_concat)\n",
    "            pred_x = self.final_layer(xout)\n",
    "            pred_z = self.final_layer(z)\n",
    "            return pred_x, pred_z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model check\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dim_embed = 44\n",
    "# log = True\n",
    "# pretrain = False\n",
    "\n",
    "# model = SCP2model(device, dim_embed, log, pretrain)\n",
    "# pred = model(x.to(device))\n",
    "# pred.shape\n",
    "# emb = model.embed_cell(torch.arange(6, device=device))\n",
    "# sim = cell @ emb.T\n",
    "# sim[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define util function\n",
    "\n",
    "def MRRMSE(pred, y, means=None, stds=None):\n",
    "      pred = pred.detach().cpu().numpy()\n",
    "      y = y.detach().cpu().numpy()\n",
    "      if means is not None:\n",
    "            pred = pred * stds + means\n",
    "            y = y * stds + means\n",
    "      return np.sqrt(np.square(y - pred).mean(axis=1)).mean()    \n",
    "\n",
    "def mrrmse_loss(pred, y):\n",
    "      return torch.sqrt(torch.square(pred - y).mean(dim=1)).mean()\n",
    "\n",
    "def compose_loss(pred, y):\n",
    "      return mrrmse_loss(pred, y) + F.smooth_l1_loss(pred, y)\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    #random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pre-training function\n",
    "\n",
    "def pretrain_model(device, dim_embed, num_layers, drop_prob, log, pretrain, optimizer, \n",
    "                   criterion, learning_rate, weight, num_epochs, batch_size, verbose):\n",
    "      # train loader\n",
    "      train_loader = DataLoader(SCPpretrain(de_train, smile, cell_type_dict, de_corr_cell, de_corr_comp),\n",
    "                                batch_size=batch_size, shuffle=True)\n",
    "      \n",
    "      # model & optimizer & criterion\n",
    "      model = SCP2model(device, dim_embed, num_layers, drop_prob, log, pretrain)\n",
    "      \n",
    "      if optimizer.lower()==\"sgd\":\n",
    "            optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.0)\n",
    "      elif optimizer.lower()=='adam':\n",
    "            optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "      else:\n",
    "            print(\"Wrong optimizer!\")\n",
    "            return\n",
    "      \n",
    "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "      \n",
    "      if criterion.lower()==\"l1\":\n",
    "            criterion = nn.SmoothL1Loss(0.5)\n",
    "      elif criterion.lower()==\"l2\":\n",
    "            criterion = nn.MSELoss(reduce=\"sum\")\n",
    "      elif criterion.lower()==\"MRRMSE\":\n",
    "            criterion = mrrmse_loss\n",
    "      elif criterion.lower() == \"compose\":\n",
    "            criterion = compose_loss\n",
    "      else:\n",
    "            print(\"Wrong criterion!\")\n",
    "            return\n",
    "      \n",
    "      # fixed values \n",
    "      ele_vals = []\n",
    "      ele_idxs = []\n",
    "      for i in range(smile.shape[0]):\n",
    "            ele_val = smile.iloc[i, :].values\n",
    "            ele_bool = ele_val != 0\n",
    "            ele_idx = [i if ele_bool[i]==True else 0 for i in np.arange(ele_val.shape[0])]\n",
    "            ele_vals.append(list(ele_val[1:]))\n",
    "            ele_idxs.append(list(ele_idx[1:]))\n",
    "      fixed_comp_vals = torch.tensor(ele_vals, device=device)\n",
    "      fixed_comp_idxs = torch.tensor(ele_idxs, device=device)\n",
    "      fixed_cells = torch.arange(6, device=device)\n",
    "      \n",
    "      model.train()\n",
    "      for epoch in tqdm(range(1, num_epochs+1)):\n",
    "            \n",
    "            train_loss = 0.\n",
    "            for x, y in train_loader:\n",
    "                  x, y = x.to(device), y.to(device)\n",
    "                  optimizer.zero_grad()\n",
    "                  \n",
    "                  x_cell, x_comp = model(x)\n",
    "                  cells = model.calc_cell_vector(fixed_cells)\n",
    "                  comps = model.calc_comp_vector(fixed_comp_idxs, fixed_comp_vals)\n",
    "                  sims = torch.cat([x_cell @ cells.T,\n",
    "                                    x_comp @ comps.T], dim=1)\n",
    "                  if weight is not None:\n",
    "                        B_idx = (x[:, 0] == 0) | (x[:, 0] == 1)\n",
    "                        loss_B = criterion(sims[B_idx], y[B_idx])\n",
    "                        loss_T = criterion(sims[~B_idx], y[~B_idx])\n",
    "                        loss = loss_B * weight + loss_T * (1-weight)\n",
    "                  else:\n",
    "                        loss = criterion(sims, y)\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "                  \n",
    "                  train_loss += loss.item()\n",
    "                  \n",
    "            scheduler.step()      \n",
    "            # print loss per epoch\n",
    "            if verbose:\n",
    "                  print(\"[Epoch : %3d] [Loss : %.6f ]\"%(epoch, train_loss/len(train_loader)))\n",
    "                              \n",
    "      return model\n",
    "\n",
    "# Define training function\n",
    "\n",
    "def train_model(device, train_size, normalize, model, dim_embed, num_layers, drop_prob, log, pretrain, \n",
    "                optimizer, criterion, learning_rate, weight, num_epochs, batch_size, verbose):\n",
    "      # train & valid loader\n",
    "      train_idx = np.random.choice(de_train.shape[0], int(de_train.shape[0]*train_size), replace=False)\n",
    "      train_loader = DataLoader(SCPset(de_train.iloc[train_idx, :], smile, cell_type_dict, normalize),\n",
    "                                batch_size=batch_size, shuffle=True)\n",
    "      valid_loader = [1]\n",
    "      if train_size < 1.:\n",
    "            valid_idx = list(set(np.arange(de_train.shape[0])) - set(train_idx))\n",
    "            valid_loader = DataLoader(SCPset(de_train.iloc[valid_idx, :], smile, cell_type_dict, normalize),\n",
    "                                      batch_size=batch_size, shuffle=True)\n",
    "      # model & optimizer & criterion\n",
    "      if model is None:\n",
    "            model = SCP2model(device, dim_embed, num_layers, drop_prob, log, pretrain)  \n",
    "      if optimizer.lower()==\"sgd\":\n",
    "            optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.01)\n",
    "      elif optimizer.lower()=='adam':\n",
    "            optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "      elif optimizer.lower()==\"adamw\":\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "      else:\n",
    "            print(\"Wrong optimizer!\")\n",
    "            return\n",
    "      \n",
    "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "      \n",
    "      if criterion.lower()==\"l1\":\n",
    "            criterion = nn.SmoothL1Loss(0.5)\n",
    "      elif criterion.lower()==\"l2\":\n",
    "            criterion = nn.MSELoss()\n",
    "      elif criterion.lower()==\"MRRMSE\":\n",
    "            criterion = mrrmse_loss\n",
    "      elif criterion.lower() == \"compose\":\n",
    "            criterion = compose_loss\n",
    "      else:\n",
    "            print(\"Wrong criterion!\")\n",
    "            return\n",
    "      \n",
    "      best_mrrmse = 10\n",
    "      model.pretrain = False\n",
    "      for epoch in tqdm(range(1, num_epochs+1)):\n",
    "            train_loss = 0.\n",
    "            valid_loss = 0.\n",
    "            train_mrrmse = 0.\n",
    "            valid_mrrmse = 0.\n",
    "            unnormal_mrrmse = 0.\n",
    "            \n",
    "            model.train()\n",
    "            for x, y in train_loader:\n",
    "                  x, y = x.to(device), y.to(device)\n",
    "                  optimizer.zero_grad()\n",
    "                  pred_x, pred_z, mu, logvar = model(x)\n",
    "                  # if verbose and F.mse_loss(pred[0,:], pred[1,:]).item() < 0.0001:\n",
    "                  #       print(\"trivial solution !, mse of prediction is %.10f\"%F.mse_loss(pred[0,:], pred[1,:]).item())\n",
    "                  loss_x = criterion(pred_x, y)\n",
    "                  loss_z = criterion(pred_z, y)\n",
    "                  loss = loss_x + loss_z\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "                  \n",
    "                  train_loss += loss.item()\n",
    "                  train_mrrmse += MRRMSE(pred_x, y)\n",
    "            if train_size < 1.:\n",
    "                  model.eval()\n",
    "                  for x, y in valid_loader:\n",
    "                        x, y = x.to(device), y.to(device)\n",
    "                        with torch.no_grad():\n",
    "                              pred, _, _, _ = model(x)\n",
    "                              loss = criterion(pred, y)\n",
    "                        \n",
    "                        valid_loss += loss.item()\n",
    "                        valid_mrrmse += MRRMSE(pred, y)\n",
    "                        if normalize:\n",
    "                              unnormal_mrrmse += MRRMSE(pred, y, valid_loader.dataset.means,\n",
    "                                                        valid_loader.dataset.stds)\n",
    "            scheduler.step()      \n",
    "            # print loss per epoch\n",
    "            if verbose:\n",
    "                  print(\"[Epoch : %3d] [Loss : %.4f / %.4f] [MRRMSE : %.3f / %.3f / %.3f]\"%(\n",
    "                        epoch, train_loss/len(train_loader), valid_loss/len(valid_loader),\n",
    "                        train_mrrmse/len(train_loader), valid_mrrmse/len(valid_loader), unnormal_mrrmse/len(valid_loader)))\n",
    "            if train_size < 1. and best_mrrmse > valid_mrrmse/len(valid_loader):\n",
    "                  best_mrrmse = valid_mrrmse/len(valid_loader)\n",
    "            \n",
    "      return model, best_mrrmse\n",
    "\n",
    "# Define inferring function\n",
    "\n",
    "def infer_model(device, model, normalize):\n",
    "      gene_means = de_train.iloc[:, 5:].mean(axis=0).values\n",
    "      gene_stds = de_train.iloc[:, 5:].std(axis=0).values\n",
    "      test_loader = DataLoader(SCPset(None, smile, cell_type_dict, normalize),\n",
    "                               batch_size=255, shuffle=False)\n",
    "      model.eval()\n",
    "      for x in test_loader:\n",
    "            x = x.to(device)\n",
    "            with torch.no_grad():\n",
    "                  pred,_,_,_ = model(x)\n",
    "                  pred = pd.DataFrame(pred.detach().cpu().numpy())\n",
    "      if normalize:\n",
    "            pred = pred * gene_stds + gene_means\n",
    "      return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiuser/.conda/envs/taeuk/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a08b3394be4bd2b3cd51fdee4e101a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch :   1] [Loss : 419.039014 ]\n",
      "[Epoch :   2] [Loss : 139.513788 ]\n",
      "[Epoch :   3] [Loss : 92.183400 ]\n",
      "[Epoch :   4] [Loss : 54.513024 ]\n",
      "[Epoch :   5] [Loss : 36.471241 ]\n",
      "[Epoch :   6] [Loss : 26.783702 ]\n",
      "[Epoch :   7] [Loss : 20.093834 ]\n",
      "[Epoch :   8] [Loss : 15.289434 ]\n",
      "[Epoch :   9] [Loss : 11.930021 ]\n",
      "[Epoch :  10] [Loss : 9.580056 ]\n",
      "[Epoch :  11] [Loss : 7.878072 ]\n",
      "[Epoch :  12] [Loss : 6.595677 ]\n",
      "[Epoch :  13] [Loss : 5.606435 ]\n",
      "[Epoch :  14] [Loss : 4.826329 ]\n",
      "[Epoch :  15] [Loss : 4.196658 ]\n",
      "[Epoch :  16] [Loss : 3.684660 ]\n",
      "[Epoch :  17] [Loss : 3.258793 ]\n",
      "[Epoch :  18] [Loss : 2.901282 ]\n",
      "[Epoch :  19] [Loss : 2.599534 ]\n",
      "[Epoch :  20] [Loss : 2.340067 ]\n",
      "[Epoch :  21] [Loss : 2.117100 ]\n",
      "[Epoch :  22] [Loss : 1.922408 ]\n",
      "[Epoch :  23] [Loss : 1.753173 ]\n",
      "[Epoch :  24] [Loss : 1.603361 ]\n",
      "[Epoch :  25] [Loss : 1.470900 ]\n",
      "[Epoch :  26] [Loss : 1.353913 ]\n",
      "[Epoch :  27] [Loss : 1.248714 ]\n",
      "[Epoch :  28] [Loss : 1.155594 ]\n",
      "[Epoch :  29] [Loss : 1.071221 ]\n",
      "[Epoch :  30] [Loss : 0.996100 ]\n",
      "[Epoch :  31] [Loss : 0.927614 ]\n",
      "[Epoch :  32] [Loss : 0.865209 ]\n",
      "[Epoch :  33] [Loss : 0.810058 ]\n",
      "[Epoch :  34] [Loss : 0.758306 ]\n",
      "[Epoch :  35] [Loss : 0.711409 ]\n",
      "[Epoch :  36] [Loss : 0.668783 ]\n",
      "[Epoch :  37] [Loss : 0.630081 ]\n",
      "[Epoch :  38] [Loss : 0.594093 ]\n",
      "[Epoch :  39] [Loss : 0.561024 ]\n",
      "[Epoch :  40] [Loss : 0.530840 ]\n",
      "[Epoch :  41] [Loss : 0.502797 ]\n",
      "[Epoch :  42] [Loss : 0.476622 ]\n",
      "[Epoch :  43] [Loss : 0.452826 ]\n",
      "[Epoch :  44] [Loss : 0.430680 ]\n",
      "[Epoch :  45] [Loss : 0.409833 ]\n",
      "[Epoch :  46] [Loss : 0.390756 ]\n",
      "[Epoch :  47] [Loss : 0.372846 ]\n",
      "[Epoch :  48] [Loss : 0.356323 ]\n",
      "[Epoch :  49] [Loss : 0.340875 ]\n",
      "[Epoch :  50] [Loss : 0.326233 ]\n",
      "[Epoch :  51] [Loss : 0.312686 ]\n",
      "[Epoch :  52] [Loss : 0.299918 ]\n",
      "[Epoch :  53] [Loss : 0.287972 ]\n",
      "[Epoch :  54] [Loss : 0.276712 ]\n",
      "[Epoch :  55] [Loss : 0.266366 ]\n",
      "[Epoch :  56] [Loss : 0.256340 ]\n",
      "[Epoch :  57] [Loss : 0.246946 ]\n",
      "[Epoch :  58] [Loss : 0.238365 ]\n",
      "[Epoch :  59] [Loss : 0.230164 ]\n",
      "[Epoch :  60] [Loss : 0.222369 ]\n",
      "[Epoch :  61] [Loss : 0.214708 ]\n",
      "[Epoch :  62] [Loss : 0.208012 ]\n",
      "[Epoch :  63] [Loss : 0.201375 ]\n",
      "[Epoch :  64] [Loss : 0.195164 ]\n",
      "[Epoch :  65] [Loss : 0.189334 ]\n",
      "[Epoch :  66] [Loss : 0.183826 ]\n",
      "[Epoch :  67] [Loss : 0.178453 ]\n",
      "[Epoch :  68] [Loss : 0.173370 ]\n",
      "[Epoch :  69] [Loss : 0.168581 ]\n",
      "[Epoch :  70] [Loss : 0.164204 ]\n",
      "[Epoch :  71] [Loss : 0.159794 ]\n",
      "[Epoch :  72] [Loss : 0.155777 ]\n",
      "[Epoch :  73] [Loss : 0.151845 ]\n",
      "[Epoch :  74] [Loss : 0.147976 ]\n",
      "[Epoch :  75] [Loss : 0.144439 ]\n",
      "[Epoch :  76] [Loss : 0.141319 ]\n",
      "[Epoch :  77] [Loss : 0.137797 ]\n",
      "[Epoch :  78] [Loss : 0.134779 ]\n",
      "[Epoch :  79] [Loss : 0.131732 ]\n",
      "[Epoch :  80] [Loss : 0.128902 ]\n",
      "[Epoch :  81] [Loss : 0.126278 ]\n",
      "[Epoch :  82] [Loss : 0.123677 ]\n",
      "[Epoch :  83] [Loss : 0.121267 ]\n",
      "[Epoch :  84] [Loss : 0.118893 ]\n",
      "[Epoch :  85] [Loss : 0.116556 ]\n",
      "[Epoch :  86] [Loss : 0.114444 ]\n",
      "[Epoch :  87] [Loss : 0.112353 ]\n",
      "[Epoch :  88] [Loss : 0.110300 ]\n",
      "[Epoch :  89] [Loss : 0.108388 ]\n",
      "[Epoch :  90] [Loss : 0.106672 ]\n",
      "[Epoch :  91] [Loss : 0.104934 ]\n",
      "[Epoch :  92] [Loss : 0.103167 ]\n",
      "[Epoch :  93] [Loss : 0.101667 ]\n",
      "[Epoch :  94] [Loss : 0.100127 ]\n",
      "[Epoch :  95] [Loss : 0.098496 ]\n",
      "[Epoch :  96] [Loss : 0.097091 ]\n",
      "[Epoch :  97] [Loss : 0.095793 ]\n",
      "[Epoch :  98] [Loss : 0.094420 ]\n",
      "[Epoch :  99] [Loss : 0.093002 ]\n",
      "[Epoch : 100] [Loss : 0.091878 ]\n",
      "[Epoch : 101] [Loss : 0.090538 ]\n",
      "[Epoch : 102] [Loss : 0.089475 ]\n",
      "[Epoch : 103] [Loss : 0.088318 ]\n",
      "[Epoch : 104] [Loss : 0.087412 ]\n",
      "[Epoch : 105] [Loss : 0.086330 ]\n",
      "[Epoch : 106] [Loss : 0.085393 ]\n",
      "[Epoch : 107] [Loss : 0.084430 ]\n",
      "[Epoch : 108] [Loss : 0.083406 ]\n",
      "[Epoch : 109] [Loss : 0.082551 ]\n",
      "[Epoch : 110] [Loss : 0.081716 ]\n",
      "[Epoch : 111] [Loss : 0.080996 ]\n",
      "[Epoch : 112] [Loss : 0.080144 ]\n",
      "[Epoch : 113] [Loss : 0.079395 ]\n",
      "[Epoch : 114] [Loss : 0.078742 ]\n",
      "[Epoch : 115] [Loss : 0.077801 ]\n",
      "[Epoch : 116] [Loss : 0.077080 ]\n",
      "[Epoch : 117] [Loss : 0.076558 ]\n",
      "[Epoch : 118] [Loss : 0.076002 ]\n",
      "[Epoch : 119] [Loss : 0.075321 ]\n",
      "[Epoch : 120] [Loss : 0.074787 ]\n",
      "[Epoch : 121] [Loss : 0.074220 ]\n",
      "[Epoch : 122] [Loss : 0.073600 ]\n",
      "[Epoch : 123] [Loss : 0.073039 ]\n",
      "[Epoch : 124] [Loss : 0.072585 ]\n",
      "[Epoch : 125] [Loss : 0.072072 ]\n",
      "[Epoch : 126] [Loss : 0.071660 ]\n",
      "[Epoch : 127] [Loss : 0.071068 ]\n",
      "[Epoch : 128] [Loss : 0.070624 ]\n",
      "[Epoch : 129] [Loss : 0.070360 ]\n",
      "[Epoch : 130] [Loss : 0.069715 ]\n",
      "[Epoch : 131] [Loss : 0.069322 ]\n",
      "[Epoch : 132] [Loss : 0.068993 ]\n",
      "[Epoch : 133] [Loss : 0.068721 ]\n",
      "[Epoch : 134] [Loss : 0.068181 ]\n",
      "[Epoch : 135] [Loss : 0.067910 ]\n",
      "[Epoch : 136] [Loss : 0.067510 ]\n",
      "[Epoch : 137] [Loss : 0.067235 ]\n",
      "[Epoch : 138] [Loss : 0.066861 ]\n",
      "[Epoch : 139] [Loss : 0.066637 ]\n",
      "[Epoch : 140] [Loss : 0.066237 ]\n",
      "[Epoch : 141] [Loss : 0.066010 ]\n",
      "[Epoch : 142] [Loss : 0.065843 ]\n",
      "[Epoch : 143] [Loss : 0.065496 ]\n",
      "[Epoch : 144] [Loss : 0.065481 ]\n",
      "[Epoch : 145] [Loss : 0.065040 ]\n",
      "[Epoch : 146] [Loss : 0.064835 ]\n",
      "[Epoch : 147] [Loss : 0.064530 ]\n",
      "[Epoch : 148] [Loss : 0.064441 ]\n",
      "[Epoch : 149] [Loss : 0.064101 ]\n",
      "[Epoch : 150] [Loss : 0.063937 ]\n",
      "[Epoch : 151] [Loss : 0.063837 ]\n",
      "[Epoch : 152] [Loss : 0.063493 ]\n",
      "[Epoch : 153] [Loss : 0.063387 ]\n",
      "[Epoch : 154] [Loss : 0.063170 ]\n",
      "[Epoch : 155] [Loss : 0.063031 ]\n",
      "[Epoch : 156] [Loss : 0.062864 ]\n",
      "[Epoch : 157] [Loss : 0.062814 ]\n",
      "[Epoch : 158] [Loss : 0.062518 ]\n",
      "[Epoch : 159] [Loss : 0.062548 ]\n",
      "[Epoch : 160] [Loss : 0.062293 ]\n",
      "[Epoch : 161] [Loss : 0.062175 ]\n",
      "[Epoch : 162] [Loss : 0.062096 ]\n",
      "[Epoch : 163] [Loss : 0.061971 ]\n",
      "[Epoch : 164] [Loss : 0.061900 ]\n",
      "[Epoch : 165] [Loss : 0.061912 ]\n",
      "[Epoch : 166] [Loss : 0.061744 ]\n",
      "[Epoch : 167] [Loss : 0.061626 ]\n",
      "[Epoch : 168] [Loss : 0.061681 ]\n",
      "[Epoch : 169] [Loss : 0.061510 ]\n",
      "[Epoch : 170] [Loss : 0.061314 ]\n",
      "[Epoch : 171] [Loss : 0.061184 ]\n",
      "[Epoch : 172] [Loss : 0.061147 ]\n",
      "[Epoch : 173] [Loss : 0.061205 ]\n",
      "[Epoch : 174] [Loss : 0.061019 ]\n",
      "[Epoch : 175] [Loss : 0.061111 ]\n",
      "[Epoch : 176] [Loss : 0.060981 ]\n",
      "[Epoch : 177] [Loss : 0.060967 ]\n",
      "[Epoch : 178] [Loss : 0.060870 ]\n",
      "[Epoch : 179] [Loss : 0.060874 ]\n",
      "[Epoch : 180] [Loss : 0.060941 ]\n",
      "[Epoch : 181] [Loss : 0.060755 ]\n",
      "[Epoch : 182] [Loss : 0.060799 ]\n",
      "[Epoch : 183] [Loss : 0.060791 ]\n",
      "[Epoch : 184] [Loss : 0.060691 ]\n",
      "[Epoch : 185] [Loss : 0.060743 ]\n",
      "[Epoch : 186] [Loss : 0.060562 ]\n",
      "[Epoch : 187] [Loss : 0.060662 ]\n",
      "[Epoch : 188] [Loss : 0.060699 ]\n",
      "[Epoch : 189] [Loss : 0.060688 ]\n",
      "[Epoch : 190] [Loss : 0.060597 ]\n",
      "[Epoch : 191] [Loss : 0.060756 ]\n",
      "[Epoch : 192] [Loss : 0.060547 ]\n",
      "[Epoch : 193] [Loss : 0.060613 ]\n",
      "[Epoch : 194] [Loss : 0.060599 ]\n",
      "[Epoch : 195] [Loss : 0.060540 ]\n",
      "[Epoch : 196] [Loss : 0.060478 ]\n",
      "[Epoch : 197] [Loss : 0.060560 ]\n",
      "[Epoch : 198] [Loss : 0.060528 ]\n",
      "[Epoch : 199] [Loss : 0.060722 ]\n",
      "[Epoch : 200] [Loss : 0.060680 ]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f226466af2c4465384330cfc2b32219c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1915653/3917753283.py:65: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  torch.tensor(y, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch :   1] [Loss : 2.2533 / 0.8802] [MRRMSE : 0.803 / 0.660 / 1.332]\n",
      "[Epoch :   2] [Loss : 2.2309 / 0.8876] [MRRMSE : 0.797 / 0.665 / 1.348]\n",
      "[Epoch :   3] [Loss : 2.2097 / 0.8859] [MRRMSE : 0.790 / 0.664 / 1.345]\n",
      "[Epoch :   4] [Loss : 2.1716 / 0.8999] [MRRMSE : 0.778 / 0.672 / 1.372]\n",
      "[Epoch :   5] [Loss : 2.1489 / 0.9009] [MRRMSE : 0.772 / 0.672 / 1.365]\n",
      "[Epoch :   6] [Loss : 2.1150 / 0.8890] [MRRMSE : 0.761 / 0.664 / 1.344]\n",
      "[Epoch :   7] [Loss : 2.1313 / 0.8910] [MRRMSE : 0.765 / 0.665 / 1.348]\n",
      "[Epoch :   8] [Loss : 2.1030 / 0.8924] [MRRMSE : 0.757 / 0.666 / 1.349]\n",
      "[Epoch :   9] [Loss : 2.1167 / 0.8781] [MRRMSE : 0.761 / 0.658 / 1.313]\n",
      "[Epoch :  10] [Loss : 2.0934 / 0.8700] [MRRMSE : 0.753 / 0.653 / 1.299]\n",
      "[Epoch :  11] [Loss : 2.0469 / 0.8833] [MRRMSE : 0.739 / 0.660 / 1.323]\n",
      "[Epoch :  12] [Loss : 2.0765 / 0.8837] [MRRMSE : 0.749 / 0.661 / 1.320]\n",
      "[Epoch :  13] [Loss : 2.0595 / 0.8900] [MRRMSE : 0.745 / 0.663 / 1.320]\n",
      "[Epoch :  14] [Loss : 2.0457 / 0.8718] [MRRMSE : 0.740 / 0.653 / 1.294]\n",
      "[Epoch :  15] [Loss : 2.0589 / 0.8862] [MRRMSE : 0.743 / 0.661 / 1.324]\n",
      "[Epoch :  16] [Loss : 2.0932 / 0.8969] [MRRMSE : 0.755 / 0.669 / 1.332]\n",
      "[Epoch :  17] [Loss : 2.0194 / 0.8886] [MRRMSE : 0.732 / 0.663 / 1.324]\n",
      "[Epoch :  18] [Loss : 2.0376 / 0.8678] [MRRMSE : 0.736 / 0.651 / 1.280]\n",
      "[Epoch :  19] [Loss : 2.0475 / 0.8666] [MRRMSE : 0.741 / 0.650 / 1.276]\n",
      "[Epoch :  20] [Loss : 2.0259 / 0.8720] [MRRMSE : 0.736 / 0.653 / 1.290]\n",
      "[Epoch :  21] [Loss : 2.0189 / 0.8669] [MRRMSE : 0.734 / 0.650 / 1.275]\n",
      "[Epoch :  22] [Loss : 1.9825 / 0.8711] [MRRMSE : 0.723 / 0.652 / 1.281]\n",
      "[Epoch :  23] [Loss : 1.9864 / 0.8924] [MRRMSE : 0.724 / 0.664 / 1.308]\n",
      "[Epoch :  24] [Loss : 2.0210 / 0.8873] [MRRMSE : 0.736 / 0.660 / 1.296]\n",
      "[Epoch :  25] [Loss : 2.0313 / 0.8853] [MRRMSE : 0.740 / 0.659 / 1.291]\n",
      "[Epoch :  26] [Loss : 1.9867 / 0.8682] [MRRMSE : 0.726 / 0.650 / 1.270]\n",
      "[Epoch :  27] [Loss : 1.9705 / 0.8620] [MRRMSE : 0.721 / 0.647 / 1.254]\n",
      "[Epoch :  28] [Loss : 1.9614 / 0.8861] [MRRMSE : 0.718 / 0.660 / 1.289]\n",
      "[Epoch :  29] [Loss : 1.9537 / 0.8593] [MRRMSE : 0.716 / 0.645 / 1.249]\n",
      "[Epoch :  30] [Loss : 1.9794 / 0.8633] [MRRMSE : 0.724 / 0.648 / 1.257]\n",
      "[Epoch :  31] [Loss : 1.9806 / 0.8673] [MRRMSE : 0.727 / 0.650 / 1.266]\n",
      "[Epoch :  32] [Loss : 1.9667 / 0.8694] [MRRMSE : 0.722 / 0.651 / 1.266]\n",
      "[Epoch :  33] [Loss : 1.9927 / 0.8641] [MRRMSE : 0.730 / 0.648 / 1.253]\n",
      "[Epoch :  34] [Loss : 1.9540 / 0.8906] [MRRMSE : 0.719 / 0.663 / 1.293]\n",
      "[Epoch :  35] [Loss : 1.9428 / 0.8612] [MRRMSE : 0.716 / 0.646 / 1.253]\n",
      "[Epoch :  36] [Loss : 1.9610 / 0.8724] [MRRMSE : 0.722 / 0.653 / 1.268]\n",
      "[Epoch :  37] [Loss : 1.9590 / 0.8573] [MRRMSE : 0.721 / 0.643 / 1.243]\n",
      "[Epoch :  38] [Loss : 1.9834 / 0.8567] [MRRMSE : 0.728 / 0.643 / 1.241]\n",
      "[Epoch :  39] [Loss : 1.9337 / 0.8728] [MRRMSE : 0.714 / 0.653 / 1.264]\n",
      "[Epoch :  40] [Loss : 1.9809 / 0.8647] [MRRMSE : 0.728 / 0.648 / 1.254]\n",
      "[Epoch :  41] [Loss : 1.9825 / 0.8642] [MRRMSE : 0.730 / 0.648 / 1.253]\n",
      "[Epoch :  42] [Loss : 1.9367 / 0.9004] [MRRMSE : 0.716 / 0.667 / 1.314]\n",
      "[Epoch :  43] [Loss : 1.9437 / 0.8604] [MRRMSE : 0.719 / 0.645 / 1.246]\n",
      "[Epoch :  44] [Loss : 1.9359 / 0.8787] [MRRMSE : 0.716 / 0.656 / 1.283]\n",
      "[Epoch :  45] [Loss : 1.9360 / 0.8696] [MRRMSE : 0.716 / 0.651 / 1.264]\n",
      "[Epoch :  46] [Loss : 1.9695 / 0.8823] [MRRMSE : 0.726 / 0.657 / 1.280]\n",
      "[Epoch :  47] [Loss : 1.9497 / 0.8925] [MRRMSE : 0.720 / 0.663 / 1.293]\n",
      "[Epoch :  48] [Loss : 1.9772 / 0.8660] [MRRMSE : 0.729 / 0.649 / 1.258]\n",
      "[Epoch :  49] [Loss : 1.9269 / 0.8905] [MRRMSE : 0.714 / 0.662 / 1.294]\n",
      "[Epoch :  50] [Loss : 1.9528 / 0.8653] [MRRMSE : 0.723 / 0.648 / 1.254]\n",
      "[Epoch :  51] [Loss : 1.9230 / 0.8773] [MRRMSE : 0.713 / 0.656 / 1.272]\n",
      "[Epoch :  52] [Loss : 2.0239 / 0.8768] [MRRMSE : 0.746 / 0.654 / 1.281]\n",
      "[Epoch :  53] [Loss : 1.9300 / 0.8784] [MRRMSE : 0.716 / 0.656 / 1.287]\n",
      "[Epoch :  54] [Loss : 1.9323 / 0.8607] [MRRMSE : 0.717 / 0.645 / 1.248]\n",
      "[Epoch :  55] [Loss : 1.9467 / 0.8675] [MRRMSE : 0.722 / 0.649 / 1.265]\n",
      "[Epoch :  56] [Loss : 1.9310 / 0.8683] [MRRMSE : 0.717 / 0.650 / 1.275]\n",
      "[Epoch :  57] [Loss : 1.9582 / 0.8701] [MRRMSE : 0.726 / 0.652 / 1.281]\n",
      "[Epoch :  58] [Loss : 1.9740 / 0.9016] [MRRMSE : 0.730 / 0.669 / 1.331]\n",
      "[Epoch :  59] [Loss : 1.9487 / 0.8621] [MRRMSE : 0.723 / 0.647 / 1.259]\n",
      "[Epoch :  60] [Loss : 1.9794 / 0.8573] [MRRMSE : 0.733 / 0.644 / 1.248]\n",
      "[Epoch :  61] [Loss : 1.9483 / 0.8668] [MRRMSE : 0.723 / 0.649 / 1.270]\n",
      "[Epoch :  62] [Loss : 1.9167 / 0.8791] [MRRMSE : 0.712 / 0.656 / 1.296]\n",
      "[Epoch :  63] [Loss : 1.9667 / 0.8966] [MRRMSE : 0.727 / 0.667 / 1.313]\n",
      "[Epoch :  64] [Loss : 1.9201 / 0.8618] [MRRMSE : 0.714 / 0.647 / 1.263]\n",
      "[Epoch :  65] [Loss : 1.9343 / 0.8661] [MRRMSE : 0.718 / 0.649 / 1.272]\n",
      "[Epoch :  66] [Loss : 1.9258 / 0.8676] [MRRMSE : 0.716 / 0.650 / 1.279]\n",
      "[Epoch :  67] [Loss : 1.9180 / 0.8568] [MRRMSE : 0.713 / 0.644 / 1.252]\n",
      "[Epoch :  68] [Loss : 1.9429 / 0.8584] [MRRMSE : 0.721 / 0.645 / 1.255]\n",
      "[Epoch :  69] [Loss : 1.9172 / 0.8809] [MRRMSE : 0.713 / 0.659 / 1.300]\n",
      "[Epoch :  70] [Loss : 1.9247 / 0.8880] [MRRMSE : 0.716 / 0.662 / 1.316]\n",
      "[Epoch :  71] [Loss : 1.9199 / 0.8694] [MRRMSE : 0.714 / 0.651 / 1.281]\n",
      "[Epoch :  72] [Loss : 1.9303 / 0.8566] [MRRMSE : 0.718 / 0.643 / 1.253]\n",
      "[Epoch :  73] [Loss : 1.9455 / 0.8681] [MRRMSE : 0.722 / 0.650 / 1.273]\n",
      "[Epoch :  74] [Loss : 1.9253 / 0.8737] [MRRMSE : 0.716 / 0.654 / 1.288]\n",
      "[Epoch :  75] [Loss : 1.9234 / 0.8590] [MRRMSE : 0.716 / 0.645 / 1.259]\n",
      "[Epoch :  76] [Loss : 1.9167 / 0.8726] [MRRMSE : 0.713 / 0.654 / 1.286]\n",
      "[Epoch :  77] [Loss : 2.0056 / 0.8672] [MRRMSE : 0.742 / 0.650 / 1.274]\n",
      "[Epoch :  78] [Loss : 1.9190 / 0.8623] [MRRMSE : 0.714 / 0.647 / 1.265]\n",
      "[Epoch :  79] [Loss : 1.9643 / 0.8680] [MRRMSE : 0.728 / 0.650 / 1.269]\n",
      "[Epoch :  80] [Loss : 1.9603 / 0.8608] [MRRMSE : 0.726 / 0.646 / 1.260]\n",
      "[Epoch :  81] [Loss : 1.9178 / 0.8783] [MRRMSE : 0.714 / 0.657 / 1.297]\n",
      "[Epoch :  82] [Loss : 1.9544 / 0.8601] [MRRMSE : 0.724 / 0.646 / 1.259]\n",
      "[Epoch :  83] [Loss : 1.9201 / 0.8749] [MRRMSE : 0.715 / 0.655 / 1.283]\n",
      "[Epoch :  84] [Loss : 1.9490 / 0.8902] [MRRMSE : 0.724 / 0.663 / 1.306]\n",
      "[Epoch :  85] [Loss : 1.9390 / 0.8809] [MRRMSE : 0.720 / 0.659 / 1.296]\n",
      "[Epoch :  86] [Loss : 1.9536 / 0.8812] [MRRMSE : 0.726 / 0.658 / 1.303]\n",
      "[Epoch :  87] [Loss : 1.9162 / 0.8657] [MRRMSE : 0.714 / 0.649 / 1.271]\n",
      "[Epoch :  88] [Loss : 1.9113 / 0.8618] [MRRMSE : 0.712 / 0.647 / 1.264]\n",
      "[Epoch :  89] [Loss : 1.9254 / 0.8606] [MRRMSE : 0.717 / 0.646 / 1.260]\n",
      "[Epoch :  90] [Loss : 1.9279 / 0.8597] [MRRMSE : 0.718 / 0.645 / 1.260]\n",
      "[Epoch :  91] [Loss : 1.9578 / 0.8588] [MRRMSE : 0.725 / 0.645 / 1.260]\n",
      "[Epoch :  92] [Loss : 1.9629 / 0.8812] [MRRMSE : 0.727 / 0.657 / 1.289]\n",
      "[Epoch :  93] [Loss : 1.9402 / 0.8597] [MRRMSE : 0.721 / 0.646 / 1.258]\n",
      "[Epoch :  94] [Loss : 1.9104 / 0.8678] [MRRMSE : 0.712 / 0.650 / 1.279]\n",
      "[Epoch :  95] [Loss : 1.9534 / 0.8665] [MRRMSE : 0.724 / 0.650 / 1.269]\n",
      "[Epoch :  96] [Loss : 1.9797 / 0.8679] [MRRMSE : 0.733 / 0.651 / 1.272]\n",
      "[Epoch :  97] [Loss : 1.9333 / 0.8617] [MRRMSE : 0.719 / 0.647 / 1.261]\n",
      "[Epoch :  98] [Loss : 1.9128 / 0.8607] [MRRMSE : 0.712 / 0.646 / 1.261]\n",
      "[Epoch :  99] [Loss : 2.0543 / 0.8632] [MRRMSE : 0.755 / 0.648 / 1.264]\n",
      "[Epoch : 100] [Loss : 1.9163 / 0.8596] [MRRMSE : 0.714 / 0.646 / 1.260]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "      \n",
    "# parameters\n",
    "\n",
    "fix_random_seed(231155)\n",
    "\n",
    "dim_embed = 128\n",
    "num_layers = 10\n",
    "log = True\n",
    "pretrain = True\n",
    "drop_prob = 0.3\n",
    "train_size = 0.65\n",
    "\n",
    "normalize = True\n",
    "optimizer = \"SGD\"\n",
    "criterion = \"Compose\"\n",
    "learning_rate = 0.025\n",
    "weight = None\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "optimizer_pre = \"Adam\" # Adam fix !\n",
    "criterion_pre = \"L2\"\n",
    "learning_rate_pre = 0.02\n",
    "weight_pre = None\n",
    "num_epochs_pre = 200\n",
    "batch_size_pre = 128\n",
    "\n",
    "model_pretrained = pretrain_model(device, dim_embed, num_layers, drop_prob, log, pretrain, optimizer_pre, \n",
    "                                  criterion_pre, learning_rate_pre, weight_pre, num_epochs_pre, batch_size_pre, True)\n",
    "\n",
    "model, bestmrrmse = train_model(device, train_size, normalize, model_pretrained, dim_embed, num_layers, drop_prob, log, pretrain, \n",
    "                                optimizer, criterion, learning_rate, weight, num_epochs, batch_size, True)\n",
    "# pretrain : N | normal : Y  MRRMSE : 1.491\n",
    "# pretrain : Y | normal : Y  MRRMSE : 1.334\n",
    "# pretrain : N | normal : N  MRRMSE : 1.451\n",
    "# pretrain : Y | normal : N  MRRMSE : 1.338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1BG-AS1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2M-AS1</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUP1</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.296504</td>\n",
       "      <td>0.211665</td>\n",
       "      <td>0.124898</td>\n",
       "      <td>0.135206</td>\n",
       "      <td>0.563002</td>\n",
       "      <td>0.761984</td>\n",
       "      <td>-0.013693</td>\n",
       "      <td>0.302938</td>\n",
       "      <td>-0.009029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056470</td>\n",
       "      <td>0.123769</td>\n",
       "      <td>-0.021091</td>\n",
       "      <td>0.218706</td>\n",
       "      <td>0.433305</td>\n",
       "      <td>0.326586</td>\n",
       "      <td>0.214578</td>\n",
       "      <td>0.147317</td>\n",
       "      <td>-0.136896</td>\n",
       "      <td>-0.073958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.329205</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>0.142581</td>\n",
       "      <td>0.145403</td>\n",
       "      <td>0.603104</td>\n",
       "      <td>0.828216</td>\n",
       "      <td>-0.010491</td>\n",
       "      <td>0.320782</td>\n",
       "      <td>-0.001178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059914</td>\n",
       "      <td>0.138882</td>\n",
       "      <td>-0.009893</td>\n",
       "      <td>0.233539</td>\n",
       "      <td>0.453226</td>\n",
       "      <td>0.352735</td>\n",
       "      <td>0.227589</td>\n",
       "      <td>0.154490</td>\n",
       "      <td>-0.145921</td>\n",
       "      <td>-0.071046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.369808</td>\n",
       "      <td>0.255562</td>\n",
       "      <td>0.148743</td>\n",
       "      <td>0.154635</td>\n",
       "      <td>0.681103</td>\n",
       "      <td>0.938575</td>\n",
       "      <td>-0.000567</td>\n",
       "      <td>0.347575</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.157901</td>\n",
       "      <td>0.015156</td>\n",
       "      <td>0.266242</td>\n",
       "      <td>0.500297</td>\n",
       "      <td>0.383189</td>\n",
       "      <td>0.242517</td>\n",
       "      <td>0.160406</td>\n",
       "      <td>-0.162571</td>\n",
       "      <td>-0.069888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.162650</td>\n",
       "      <td>0.128836</td>\n",
       "      <td>0.076603</td>\n",
       "      <td>0.090570</td>\n",
       "      <td>0.374698</td>\n",
       "      <td>0.474240</td>\n",
       "      <td>-0.033362</td>\n",
       "      <td>0.221444</td>\n",
       "      <td>-0.044989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024654</td>\n",
       "      <td>0.072491</td>\n",
       "      <td>-0.079114</td>\n",
       "      <td>0.131992</td>\n",
       "      <td>0.318572</td>\n",
       "      <td>0.228841</td>\n",
       "      <td>0.177377</td>\n",
       "      <td>0.121536</td>\n",
       "      <td>-0.086626</td>\n",
       "      <td>-0.084828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.368679</td>\n",
       "      <td>0.253313</td>\n",
       "      <td>0.150887</td>\n",
       "      <td>0.153933</td>\n",
       "      <td>0.677635</td>\n",
       "      <td>0.934190</td>\n",
       "      <td>-0.001269</td>\n",
       "      <td>0.347952</td>\n",
       "      <td>0.010284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069004</td>\n",
       "      <td>0.157996</td>\n",
       "      <td>0.016334</td>\n",
       "      <td>0.264738</td>\n",
       "      <td>0.497681</td>\n",
       "      <td>0.381771</td>\n",
       "      <td>0.241920</td>\n",
       "      <td>0.159662</td>\n",
       "      <td>-0.161785</td>\n",
       "      <td>-0.069368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 18212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      A1BG  A1BG-AS1       A2M   A2M-AS1     A2MP1    A4GALT      AAAS  \\\n",
       "0   0  0.296504  0.211665  0.124898  0.135206  0.563002  0.761984 -0.013693   \n",
       "1   1  0.329205  0.228900  0.142581  0.145403  0.603104  0.828216 -0.010491   \n",
       "2   2  0.369808  0.255562  0.148743  0.154635  0.681103  0.938575 -0.000567   \n",
       "3   3  0.162650  0.128836  0.076603  0.090570  0.374698  0.474240 -0.033362   \n",
       "4   4  0.368679  0.253313  0.150887  0.153933  0.677635  0.934190 -0.001269   \n",
       "\n",
       "       AACS     AAGAB  ...      ZUP1      ZW10    ZWILCH     ZWINT      ZXDA  \\\n",
       "0  0.302938 -0.009029  ...  0.056470  0.123769 -0.021091  0.218706  0.433305   \n",
       "1  0.320782 -0.001178  ...  0.059914  0.138882 -0.009893  0.233539  0.453226   \n",
       "2  0.347575  0.010046  ...  0.068700  0.157901  0.015156  0.266242  0.500297   \n",
       "3  0.221444 -0.044989  ...  0.024654  0.072491 -0.079114  0.131992  0.318572   \n",
       "4  0.347952  0.010284  ...  0.069004  0.157996  0.016334  0.264738  0.497681   \n",
       "\n",
       "       ZXDB      ZXDC    ZYG11B       ZYX     ZZEF1  \n",
       "0  0.326586  0.214578  0.147317 -0.136896 -0.073958  \n",
       "1  0.352735  0.227589  0.154490 -0.145921 -0.071046  \n",
       "2  0.383189  0.242517  0.160406 -0.162571 -0.069888  \n",
       "3  0.228841  0.177377  0.121536 -0.086626 -0.084828  \n",
       "4  0.381771  0.241920  0.159662 -0.161785 -0.069368  \n",
       "\n",
       "[5 rows x 18212 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = infer_model(device, model, normalize)\n",
    "\n",
    "df = pred.reset_index()\n",
    "df.columns = submission.columns\n",
    "display(df.head())\n",
    "\n",
    "# save\n",
    "\n",
    "title = f\"scp2_Dim{dim_embed}_Lay{num_layers}_Nor{normalize}_Log{log}_Opt{optimizer}_Cri{criterion}_lr{learning_rate}_B{batch_size}_E{num_epochs}\"\n",
    "#df.to_csv(\"/home/aiuser/taeuk/%s.csv\"%title, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "\n",
    "# model_pretrained = pretrain_model(device, dim_embed, num_layers, drop_prob, log, pretrain, optimizer_pre, \n",
    "#                                   criterion_pre, learning_rate_pre, weight_pre, num_epochs_pre, batch_size_pre, True)\n",
    "# submit, _ = train_model(device, train_size, normalize, model_pretrained, dim_embed, num_layers, drop_prob, log, pretrain, \n",
    "#                                 optimizer, criterion, learning_rate, weight, num_epochs, batch_size, True)\n",
    "# pred = infer_model(device, submit, normalize)\n",
    "\n",
    "# df_submit = pred.reset_index()\n",
    "# df_submit.columns = submission.columns\n",
    "# display(df_submit.head())\n",
    "\n",
    "# save\n",
    "\n",
    "# title = f\"scp2_Dim{dim_embed}_Lay{num_layers}_Nor{normalize}_Log{log}_Opt{optimizer}_Cri{criterion}_lr{learning_rate}_B{batch_size}_E{num_epochs}\"\n",
    "# df_submit.to_csv(\"/home/aiuser/taeuk/%s.csv\"%title, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taeuk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
